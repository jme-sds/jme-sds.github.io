[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "John Michael Epperson",
    "section": "",
    "text": "Hello, I’m John Michael Epperson\nI am a Data Scientist and Physicist based in Charlottesville, VA. My work sits at the intersection of rigorous statistical modeling and modern deep learning. Currently, I am pursuing my M.S. in Data Science at the University of Virginia, where I focus on building AI systems that are not just theoretically interesting, but practically useful. What I Do I specialize in Deep Learning, Generative AI, and Computer Vision. My background in Physics has instilled in me a “first-principles” approach to data—I don’t just run models; I try to understand the underlying mechanics of the system I am studying. My professional background includes designing automated ETL pipelines, building custom R libraries for industrial testing, and managing data quality in highly regulated environments. I enjoy the challenge of taking messy, real-world data and architecting the databases and workflows required to turn it into insight. Featured Projects My recent research focuses on applying AI to civic and environmental challenges: • RAG for Civic Engagement: I developed a Retrieval-Augmented Generation pipeline that allows residents to query the Charlottesville municipal code using natural language. The system benchmarks competitively against industry standards (LegalBenchRAG). • Computer Vision for Ecology: As part of the PlantCLEF 2025 challenge, I implemented self-supervised Vision Transformers (ViT) to tackle the difficult problem of multi-label plant classification, using advanced tiling strategies to detect species in complex environments. • Humanitarian AI: I have worked on machine learning models designed to detect blue tarps in aerial imagery, aiding in rapid disaster assessment and relief efforts. The Analog & The Digital When I step away from the GPU, I tend to go to one of two extremes: purely analog or deeply networked. Film Photography I shoot on a Minolta SRT 101, primarily using Kodak Gold 200. There is something satisfying about the mechanical nature of the camera and the patience required by film—it’s a refreshing contrast to the instant feedback of model training. Home Lab & Self-Hosting I am an avid self-hoster and network tinkerer, managing a distributed fleet of hardware to ensure data sovereignty and network security: • The Server (Repurposed Laptop): My primary node running Docker containers for Nextcloud (private cloud storage) and Pi-hole (network-wide ad/tracker blocking). • Raspberry Pi 3B: Dedicated to BirdNET-Pi, an acoustic monitoring station that uses machine learning to identify and log local bird species. • Orange Pi 3 Zero: Configured as a dedicated hotspot bridge. It creates a Wi-Fi access point that routes traffic from connected edge devices (which cannot install Tailscale natively) directly through my secure tailnet. • Networking: I utilize Tailscale to knit these devices together, allowing secure remote access without exposing ports to the public internet."
  },
  {
    "objectID": "cville_rag.html",
    "href": "cville_rag.html",
    "title": "Michael's DS Hub",
    "section": "",
    "text": "This assistant uses Retrieval Augmented Generation to answer questions about anything concerning city ordinance in Charlottesville, VA. It uses Qwen3-Embedding-0.6B for retrieval and Qwen2.5-7B-Instruct for generation.\nTry it out! This app runs on HuggingFace’s ZeroGPU.\n\n\nDisclaimer: AI makes mistakes, check important information. This is not a substitute for legal advice."
  },
  {
    "objectID": "cville_rag.html#charlottesville-law-assistant",
    "href": "cville_rag.html#charlottesville-law-assistant",
    "title": "Michael's DS Hub",
    "section": "",
    "text": "This assistant uses Retrieval Augmented Generation to answer questions about anything concerning city ordinance in Charlottesville, VA. It uses Qwen3-Embedding-0.6B for retrieval and Qwen2.5-7B-Instruct for generation.\nTry it out! This app runs on HuggingFace’s ZeroGPU.\n\n\nDisclaimer: AI makes mistakes, check important information. This is not a substitute for legal advice."
  },
  {
    "objectID": "cville_rag.html#introduction",
    "href": "cville_rag.html#introduction",
    "title": "Michael's DS Hub",
    "section": "1. Introduction",
    "text": "1. Introduction\nLocal laws are often written in dense legal terminology that the average person struggles to interpret, turning simple questions about parking or zoning into a maze of irrelevant sections and complex jargon. While current Large Language Models (LLMs) like ChatGPT have seen municipal codes, they are trained on codes from across the country, leading to generalized answers that may blend details from different jurisdictions and hallucinate non-existent regulations. To solve this, I developed the Charlottesville Local Ordinance Assistant, a system designed specifically to answer questions about Charlottesville, VA municipal code in plain English. This project utilizes a Retrieval-Augmented Generation (RAG) pipeline to ensure legal accuracy by retrieving up-to-date ordinances, coupled with a specific system prompt designed to translate that “legalese” into clear, accessible language without the need for computationally expensive fine-tuning. The results demonstrate that constraining the model to local data and utilizing strong prompt engineering significantly reduces hallucinations compared to off-the-shelf generalist models."
  },
  {
    "objectID": "cville_rag.html#data",
    "href": "cville_rag.html#data",
    "title": "Michael's DS Hub",
    "section": "2. Data",
    "text": "2. Data\nFor the RAG pipeline, the knowledge base consists of the unedited Charlottesville Municipal Code text, scraped and pre-processed from Municode. These chunks were not rephrased, ensuring that the retrieval mechanism pulls the exact letter of the law. To evaluate the RAG pipeline, I utilized a set of questions and answers generated from the original sections of the municipal code to validate retrieval accuracy (checking if the retrieved node matched the ground truth node for a given query). This dataset can be found at jme-datasci/charlottesville_qa."
  },
  {
    "objectID": "cville_rag.html#methodology",
    "href": "cville_rag.html#methodology",
    "title": "Michael's DS Hub",
    "section": "3. Methodology",
    "text": "3. Methodology\nFor the RAG methodology, I implemented a dense retrieval system. I selected Qwen3-Embedding-0.6B as the embedding model due to the relatively small size of the RAG corpus (municipal code). This model allows for high-precision retrieval without the latency of larger embedding models. The retrieved context is passed to the Qwen2.5-7B-Instruct generator to synthesize the final answer.\nThe generation model uses a maximum token count of 500 for responses where more explanation may be required and a temperature of 0.9 to encourage more factual responses. The embedding model is set to retreive 3 documents from the FAISS index using cosine similarity."
  },
  {
    "objectID": "cville_rag.html#evaluation",
    "href": "cville_rag.html#evaluation",
    "title": "Michael's DS Hub",
    "section": "4. Evaluation",
    "text": "4. Evaluation\n\nBenchmark Results\nTo strictly evaluate the legal reasoning and retrieval capabilities of the model, I utilized two established benchmarks: LegalBench-RAG, RAGBench, and my own custom dataset. I chose these because they specifically target the weaknesses of legal LLMs: the ability to reason over specific documents and the frequency of hallucinations.\nThe LegalBench-RAG, RAGBench, and my custom test split were all evaluated using meta-llama/Llama-3.1-8B as judge for seven different metrics:\n\nContext Relevance: Measures the proportion of retrieved information that is actually pertinent to the user’s query.\nContext Recall: Assesses if the retrieved context contains all the necessary ground-truth information required to answer.\nChunk Relevance: Evaluates the precision of individual retrieved document segments relative to the input query.\nFaithfulness: Checks if the generated answer is factually derived solely from the retrieved context (hallucination detection).\nAnswer Relevance: Determines how well the generated response directly addresses the user’s original prompt.\nAnswer Correctness: Scores the accuracy of the generated answer against a known gold-standard reference.\nAnswer Completeness: Checks if the response addresses all parts of the query without omitting key details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\n\nLegalBench-RAG\n\n\nCharlottesville Municipal Code\n\n\nRAGBench\n\n\n\n\n\nModel\nQwen\nLlama\nMistral\nQwen\nLlama\nMistral\nQwen\nLlama\nMistral\n\n\nContext Relevance\n87.14\n87.27\n87.23\n84.54\n84.65\n84.54\n22.47\n22.14\n22.00\n\n\nContext Recall\n72.31\n71.85\n71.92\n42.74\n43.23\n42.65\n20.43\n20.63\n20.79\n\n\nChunk Relevance\n85.62\n85.72\n85.68\n75.57\n75.47\n75.60\n24.39\n24.00\n24.31\n\n\nFaithfullness\n87.11\n81.50\n84.71\n83.65\n81.88\n82.99\n83.78\n80.73\n79.33\n\n\nAnswer Relevance\n92.17\n88.80\n91.31\n88.88\n87.33\n87.70\n86.42\n79.37\n85.31\n\n\nAnswer Correctness\n71.15\n67.94\n56.24\n60.73\n60.79\n60.40\n25.09\n16.78\n20.86\n\n\nAnswer Completeness\n89.99\n87.22\n88.70\n86.88\n83.54\n83.12\n85.64\n80.84\n82.69\n\n\n\nI compared my primary model (Qwen2.5-7B-Instruct) against Mistral-7B-Instruct-v0.3 and Llama-3.1-8B-Instruct, two similarly sized instruction tuned generation models. The results in the table above show that all models performed comparatively in the context retrieval tasks, which is expected since they all used the same embedding model, Qwen3-Embedding-0.6B. However, Qwen2.5-7B-Instruct wins in almost every generation-based metric. The 7B Qwen model shows remarkably better resistance to hallucination and ability to answer with more relevance, accuracy and completeness."
  },
  {
    "objectID": "cville_rag.html#usage-and-intended-uses",
    "href": "cville_rag.html#usage-and-intended-uses",
    "title": "Michael's DS Hub",
    "section": "5. Usage and Intended Uses",
    "text": "5. Usage and Intended Uses\nThe intended use case for this model is to assist residents of Charlottesville, VA, in understanding local ordinances regarding zoning, parking, and noise complaints without needing a legal background. It is not a replacement for a lawyer but rather a tool for accessibility.\nBelow is an example of how the RAG pipeline class is constructed and used to generate responses with retrieval.\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModel\nimport faiss as fai\nfrom langchain_community.vectorstores import  FAISS\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\n\nclass MyRAGPipeline:\n    def __init__(self, model_name: str, embedding_model_name: str, vector_db_path: str):\n        self.embedding_model_name = embedding_model_name\n        self.max_new_tokens = 500\n        \n        print(f\"Loading Model: {model_name}...\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n        \n        # --- CRITICAL: Load to CPU first ---\n        # ZeroGPU does not have a GPU available during global startup.\n        # We load the weights into System RAM now, and move them to GPU later.\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name, \n            device_map=\"cpu\",  # Force CPU loading\n            torch_dtype=torch.bfloat16, \n            token=HF_TOKEN\n        )\n        \n        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n        self.tokenizer.padding_side = \"left\"\n        \n        print(\"Loading Embeddings...\")\n        self.embedding_model = HuggingFaceEmbeddings(\n            model_name=self.embedding_model_name,\n            model_kwargs={\"device\": \"cpu\"}, # Keep embeddings on CPU\n            encode_kwargs={\"normalize_embeddings\": True},\n        )     \n\n        print(f\"Loading Vector DB from {vector_db_path}...\")\n        if not os.path.exists(vector_db_path):\n             raise FileNotFoundError(f\"Could not find vector DB at {vector_db_path}. Please upload your 'index' folder.\")\n             \n        self.vector_db = FAISS.load_local(vector_db_path, self.embedding_model, allow_dangerous_deserialization=True)\n        print(\"RAG Pipeline Initialized (CPU Mode)\")\n\n    def retrieve(self, query, num_docs=3):\n        return self.vector_db.similarity_search(query, k=num_docs)\n\n    def _format_prompt(self, query, retrieved_docs):\n        # 1. Build Context\n        context = \"Extracted documents:\\n\"\n        for doc in retrieved_docs:\n            section = doc.metadata.get('Section', 'N/A')\n            subtitle = doc.metadata.get('Subtitle', 'Context')\n            context += f\"{section} - {subtitle}:::\\n{doc.page_content}\\n\\n\"\n\n        # 2. Universal Chat Template (Works for Qwen, Llama, Mistral, etc.)\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are a helpful legal interpreter. Use the following context to answer the user's question.\\nContext:\\n{context}\"\n            },\n            {\n                \"role\": \"system\",\n                \"content\": \"Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked. Your response should be concise and relevant to the question. Always provide the section number and title of the source document. Also please use plain English when responding, not legal jargon. \\n Now answer the following question.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": query\n            }\n        ]\n        \n        # This applies the correct format for WHATEVER model you are using\n        prompt = self.tokenizer.apply_chat_template(\n            messages, \n            tokenize=False, \n            add_generation_prompt=True\n        )\n        return prompt\n\n    def generate(self, query, num_docs=3):\n        # 1. Retrieve\n        retrieved_docs = self.retrieve(query, num_docs)\n        \n        # 2. Format Prompt\n        prompt_str = self._format_prompt(query, retrieved_docs)\n        \n        # 3. Tokenize\n        inputs = self.tokenizer(prompt_str, return_tensors=\"pt\").to(self.model.device)\n        \n        # 4. Generate (Streaming is simpler for direct model usage, but here we do blocking)\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=self.max_new_tokens,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n            \n        # 5. Decode\n        # Slicing [input_len:] ensures we only return the new text, not the prompt\n        input_len = inputs.input_ids.shape[1]\n        generated_text = self.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n        \n        return generated_text\n\n# --- INITIALIZATION ---\n# Using standard paths and models\nMODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\nEMBEDDING_NAME = 'Qwen/Qwen3-Embedding-0.6B'\nVECDB_PATH = 'index/'\n\nrag = MyRAGPipeline(model_name, embedding_name, vecdb_path)\n\nprompt = \"My neighbor is playing loud music on their porch. What time does the 'quiet period' start, and what is the maximum decibel level allowed in a residential zone?\"\n\n\nprint(rag.generate(prompt))"
  },
  {
    "objectID": "cville_rag.html#prompt-format",
    "href": "cville_rag.html#prompt-format",
    "title": "Michael's DS Hub",
    "section": "Prompt Format",
    "text": "Prompt Format\nThe model relies on a strict system prompt to ensure the output is simplified but factually accurate. The prompt injects the retrieved RAG context directly into the system message.\nYou are a helpful legal interpreter.\n        You are given the following context:\n        {context}\\n\\n\n        Using the information contained in the context,\n        give a comprehensive answer to the question.\n        Respond only to the question asked. Your response should be concise and relevant to the question.\n        Always provide the section number and title of the source document.\n        Also please use plain English when responding, not legal jargon.\n        \n        Question: {query}\""
  },
  {
    "objectID": "cville_rag.html#expected-output-format",
    "href": "cville_rag.html#expected-output-format",
    "title": "Michael's DS Hub",
    "section": "Expected Output Format",
    "text": "Expected Output Format\nThe model is expected to output a plain-English translation of the input text, simplifying sentence structure while retaining critical entities (dates, fines, locations).\nAccording to document &lt;Section number&gt;, the Clerk of the Council is responsible for keeping the city's official seal. \nThey must stamp this seal on any papers or documents when the Council's laws \nor decisions require it."
  },
  {
    "objectID": "cville_rag.html#limitations",
    "href": "cville_rag.html#limitations",
    "title": "Michael's DS Hub",
    "section": "Limitations",
    "text": "Limitations\nThe primary limitation of this model is that while it reduces hallucinations, it does not eliminate them; users should verify important legal details with the official Municode source. Additionally, the model is strictly limited to the Charlottesville context; applying it to Albemarle County or other jurisdictions will result in incorrect information. Finally, because the model was not fine-tuned, it may occasionally slip back into dense terminology if the retrieved ordinance is exceptionally complex."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "portfolio",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]